---
integrations:
  - name: nri-flex
    # interval: 30s
    config:
      name: confluentKafkaFlex
      global:
        base_url: https://api.telemetry.confluent.cloud
        user: <user@example.com>
        pass: <password>
        headers:
          accept: application/json
      apis:
        # https://docs.confluent.io/current/cloud/metrics-api.html
        - event_type: confluentKafkaSample
          url: /v1/metrics/cloud/query
          rename_keys:
            value: received_bytes
            metric.label.topic: topic_name
          method: POST
          payload: >
            {
                "aggregations": [
                    {
                        "agg": "SUM",
                        "metric": "io.confluent.kafka.server/received_bytes/delta"
                    }
                ],
                "filter": {
                    "filters": [
                        {
                            "field": "metric.label.cluster_id",
                            "op": "EQ",
                            "value": "<clusterID>"
                        }
                    ],
                    "op": "AND"
                },
                "granularity": "PT1M",
                "group_by": [
                    "metric.label.topic"
                ],
                "intervals": [
                    "${timestamp:datetimeutctz-2min}/${timestamp:datetimeutctz-1min}"
                ],
                "limit": 1000
            }
        - event_type: confluentKafkaSample
          url: /v1/metrics/cloud/query
          rename_keys:
            value: retained_bytes
            metric.label.topic: topic_name
          method: POST
          payload: >
            {
                "aggregations": [
                    {
                        "agg": "SUM",
                        "metric": "io.confluent.kafka.server/retained_bytes"
                    }
                ],
                "filter": {
                    "filters": [
                        {
                            "field": "metric.label.cluster_id",
                            "op": "EQ",
                            "value": "<clusterID>"
                        }
                    ],
                    "op": "AND"
                },
                "granularity": "PT1M",
                "group_by": [
                    "metric.label.topic"
                ],
                "intervals": [
                    "${timestamp:datetimeutctz-2min}/${timestamp:datetimeutctz-1min}"
                ],
                "limit": 1000
            }
        - event_type: confluentKafkaSample
          url: /v1/metrics/cloud/query
          rename_keys:
            value: sent_bytes
            metric.label.topic: topic_name
          method: POST
          payload: >
            {
                "aggregations": [
                    {
                        "agg": "SUM",
                        "metric": "io.confluent.kafka.server/sent_bytes/delta"
                    }
                ],
                "filter": {
                    "filters": [
                        {
                            "field": "metric.label.cluster_id",
                            "op": "EQ",
                            "value": "<clusterID>"
                        }
                    ],
                    "op": "AND"
                },
                "granularity": "PT1M",
                "group_by": [
                    "metric.label.topic"
                ],
                "intervals": [
                    "${timestamp:datetimeutctz-2min}/${timestamp:datetimeutctz-1min}"
                ],
                "limit": 1000
            }
        - event_type: confluentKafkaLagSample
          commands:
            # Requires the installation of Java and Confluent Platform to work
            # https://docs.confluent.io/current/cloud/using/monitor-lag.html
            - run: <pathToConfluentPlatform>/bin/kafka-consumer-groups --bootstrap-server "<bootstrapEndpoint>" --command-config client-ssl.properties --describe --group my-group
              line_start: 6
              split: horizontal
              split_by: \s+
              set_header:
                [
                  GROUP,
                  TOPIC,
                  PARTITION,
                  CURRENT-OFFSET,
                  LOG-END-OFFSET,
                  LAG,
                  CONSUMER-ID,
                  HOST,
                  CLIENT-ID,
                ]
